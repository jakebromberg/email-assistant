"""Local semantic classifier trained on Claude-generated labels."""

import json
import pickle
from datetime import datetime
from pathlib import Path

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
from sqlalchemy.orm import Session

from ..database.schema import EmailFeatures, SemanticLabel
from ..utils import get_logger

logger = get_logger(__name__)


class SemanticClassifier:
    """
    Local classifier for semantic email labels.

    Trained on embeddings with labels generated by Claude API.
    Provides fast, private inference after initial training.

    Attributes:
        classifier: Trained sklearn classifier
        label_encoder: Encodes string labels to integers
        model_path: Path where model is saved

    Example:
        >>> classifier = SemanticClassifier()
        >>> classifier.load("models/semantic_classifier.pkl")
        >>> label, confidence = classifier.predict(embedding)
    """

    def __init__(self):
        """Initialize classifier."""
        self.classifier: LogisticRegression | None = None
        self.label_encoder: LabelEncoder | None = None
        self.model_path: str | None = None
        self.metadata: dict = {}

    def train(
        self,
        session: Session,
        min_samples_per_class: int = 3,
        test_size: float = 0.2
    ) -> dict:
        """
        Train classifier on Claude-generated labels.

        Args:
            session: Database session
            min_samples_per_class: Minimum samples needed per label
            test_size: Fraction of data for validation

        Returns:
            Dictionary with training metrics
        """
        logger.info("Loading training data...")

        # Get semantic labels with embeddings
        results = session.query(
            SemanticLabel.label,
            EmailFeatures.subject_embedding
        ).join(
            EmailFeatures,
            SemanticLabel.message_id == EmailFeatures.message_id
        ).filter(
            EmailFeatures.subject_embedding.isnot(None)
        ).all()

        if not results:
            raise ValueError("No training data found. Run generate_semantic_labels.py first.")

        # Filter to labels with enough samples
        from collections import Counter
        label_counts = Counter(r[0] for r in results)
        valid_labels = {
            label for label, count in label_counts.items()
            if count >= min_samples_per_class
        }

        logger.info(f"Found {len(results)} total samples")
        logger.info(f"Labels with >= {min_samples_per_class} samples: {len(valid_labels)}")

        # Filter data
        filtered = [(label, emb) for label, emb in results if label in valid_labels]

        if len(filtered) < 10:
            raise ValueError(f"Not enough training data. Need at least 10 samples, got {len(filtered)}")

        # Prepare arrays
        labels = [r[0] for r in filtered]
        embeddings = np.array([r[1] for r in filtered])

        logger.info(f"Training on {len(labels)} samples with {len(valid_labels)} classes")

        # Encode labels
        self.label_encoder = LabelEncoder()
        y = self.label_encoder.fit_transform(labels)

        # Split train/test
        from sklearn.model_selection import train_test_split
        X_train, X_test, y_train, y_test = train_test_split(
            embeddings, y, test_size=test_size, random_state=42, stratify=y
        )

        # Train classifier
        logger.info("Training logistic regression...")
        self.classifier = LogisticRegression(
            max_iter=1000,
            class_weight='balanced',
            random_state=42
        )
        self.classifier.fit(X_train, y_train)

        # Evaluate
        from sklearn.metrics import accuracy_score, classification_report
        y_pred = self.classifier.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)

        logger.info(f"Validation accuracy: {accuracy:.3f}")

        # Store metadata
        self.metadata = {
            'trained_at': datetime.now().isoformat(),
            'n_samples': len(labels),
            'n_classes': len(valid_labels),
            'classes': list(self.label_encoder.classes_),
            'accuracy': accuracy,
            'min_samples_per_class': min_samples_per_class,
        }

        # Get unique labels in test set for report
        unique_test_labels = np.unique(np.concatenate([y_test, y_pred]))
        test_label_names = self.label_encoder.inverse_transform(unique_test_labels)

        return {
            'accuracy': accuracy,
            'n_samples': len(labels),
            'n_classes': len(valid_labels),
            'report': classification_report(
                y_test, y_pred,
                labels=unique_test_labels,
                target_names=test_label_names,
                output_dict=True,
                zero_division=0
            )
        }

    def predict(self, embedding: list | np.ndarray) -> tuple[str, float]:
        """
        Predict label for an embedding.

        Args:
            embedding: 384-dim embedding vector

        Returns:
            Tuple of (label, confidence)
        """
        if self.classifier is None or self.label_encoder is None:
            raise ValueError("Model not loaded. Call load() or train() first.")

        embedding = np.array(embedding).reshape(1, -1)
        proba = self.classifier.predict_proba(embedding)[0]
        predicted_class = np.argmax(proba)
        confidence = proba[predicted_class]

        label = self.label_encoder.inverse_transform([predicted_class])[0]
        return label, float(confidence)

    def predict_top_k(
        self,
        embedding: list | np.ndarray,
        k: int = 3
    ) -> list[tuple[str, float]]:
        """
        Get top-k predictions with confidence scores.

        Args:
            embedding: 384-dim embedding vector
            k: Number of top predictions to return

        Returns:
            List of (label, confidence) tuples, sorted by confidence
        """
        if self.classifier is None or self.label_encoder is None:
            raise ValueError("Model not loaded. Call load() or train() first.")

        embedding = np.array(embedding).reshape(1, -1)
        proba = self.classifier.predict_proba(embedding)[0]

        # Get top-k indices
        top_indices = np.argsort(proba)[-k:][::-1]

        results = []
        for idx in top_indices:
            label = self.label_encoder.inverse_transform([idx])[0]
            confidence = float(proba[idx])
            results.append((label, confidence))

        return results

    def save(self, path: str) -> None:
        """
        Save trained model to disk.

        Args:
            path: Path to save model (e.g., "models/semantic_classifier.pkl")
        """
        if self.classifier is None:
            raise ValueError("No model to save. Train a model first.")

        model_data = {
            'classifier': self.classifier,
            'label_encoder': self.label_encoder,
            'metadata': self.metadata,
        }

        Path(path).parent.mkdir(parents=True, exist_ok=True)
        with open(path, 'wb') as f:
            pickle.dump(model_data, f)

        # Also save metadata as JSON for easy inspection
        metadata_path = path.replace('.pkl', '_metadata.json')
        with open(metadata_path, 'w') as f:
            json.dump(self.metadata, f, indent=2)

        self.model_path = path
        logger.info(f"Model saved to {path}")

    def load(self, path: str) -> None:
        """
        Load trained model from disk.

        Args:
            path: Path to saved model
        """
        with open(path, 'rb') as f:
            model_data = pickle.load(f)

        self.classifier = model_data['classifier']
        self.label_encoder = model_data['label_encoder']
        self.metadata = model_data.get('metadata', {})
        self.model_path = path

        logger.info(f"Model loaded from {path}")
        logger.info(f"Classes: {len(self.label_encoder.classes_)}")

    def get_classes(self) -> list[str]:
        """Get list of all label classes."""
        if self.label_encoder is None:
            return []
        return list(self.label_encoder.classes_)
